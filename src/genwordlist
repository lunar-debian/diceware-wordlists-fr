#!/usr/bin/env python3

import argparse
import csv
import itertools
import locale
import math
import operator
import re
import sys

INSULT_LEMMES = {
    'bamboula',
    'bicot',
    'boche',
    'con',
    'connard',
    'connasse',
    'ducon',
    'enculé',
    'fifi',
    'fritz',
    'gouine',
    'négro',
    'pédé',
    'putain',
    'pute',
    'salaud',
    'salopard',
    'salope',
    'trouduc',
    'youpin',
}
VULGAR_LEMMES = {
    'baise',
    'baiser',
    'baiseur',
    'bite',
    'burne',
    'chier',
    'couille',
    'cul',
    'emmerde',
    'emmerder',
    'enculer',
    'foutre',
    'merde',
    'merder',
    'merdeux',
    'niquer',
    'sodomie',
}
MISC_LEMMES = {
    'comac',
    'frac',
    'viol',
    'violer',
    'suicide',
    'suicider',
    'mort',
    'paros',
    'ouzo',
}


def warn(message):
    print('Warning: {}'.format(message), file=sys.stderr)


def load_lexi_file(file, min_letters=0, max_letters=100, valid_chars='.',
                   skip_lemmes=set(), skip_cgrams=[], min_freq=0.0,
                   skip_low_lemmes=True, non_ascii_cost=0, exclude_file=None):
    data = []
    reader = csv.DictReader(file, delimiter='\t', quoting=csv.QUOTE_NONE)

    trtable = str.maketrans('îû', 'iu')

    exclude = []

    if exclude_file:
        exclude = {word.strip() for word in exclude_file}

    exclude_ends = re.compile(r'(ie|is|es|od)$')
    exclude_ver_re = re.compile(r'(cnd|sub|imp|fut|pas|2s|1p|2p|3p)')

    for row in reader:
        word = row['1_ortho']
        lemme = row['3_lemme']
        cgram = row['4_cgram']
        genre = row['5_genre']
        nombre = row['6_nombre']
        freqlemfilm = float(row['7_freqlemfilms2'])
        freqlembook = float(row['8_freqlemlivres'])
        freqfilm = float(row['9_freqfilms2'])
        freqbook = float(row['10_freqlivres'])
        infover = row['11_infover']
        nbletters = int(row['15_nblettres'])

        if nombre == 'p':
            continue

        if nbletters < min_letters or nbletters > max_letters:
            continue

        if exclude_ends.match(word):
            continue

        if lemme in skip_lemmes:
            continue

        if cgram in skip_cgrams:
            continue

        if exclude_ver_re.search(infover):
            continue

        # Application sauvage de la réforme 1990 sur l'accent circonflexe
        if 'sub:imp' not in infover and 'ind:pas' not in infover:
            word = word.translate(trtable)
            lemme = lemme.translate(trtable)

        if word in exclude:
            continue

        if re.fullmatch(valid_chars + '+', word) is None:
            continue

        if non_ascii_cost:
            cost = nbletters + len(re.findall('[^a-z]', word)) * non_ascii_cost
            if cost > max_letters:
                continue

        if (skip_low_lemmes and
                (freqlemfilm <= 0.01 or freqlembook <= 0.01)):
            continue

        freq = (2*freqfilm + freqbook) / 3
        if freq < min_freq:
            continue

        freqlem = (2*freqlemfilm + freqlembook) / 3

        data.append({
            'cgram': cgram,
            'freq': freq,
            'freqlem': freqlem,
            'lemme': lemme,
            'mot': word,
            'nbletters': nbletters,
            'phon': row['2_phon'],
        })

    return data


def clean_lemmes(data, max_lemme_variants=None):
    """Pour chaque (lemme, cgram), on conserve un seul homophone
    le lemme lui-meme ou la forme la plus frequente de l'homophone"""

    new_data = []

    data.sort(key=operator.itemgetter('lemme', 'cgram'))

    for _, words in itertools.groupby(data, key=operator.itemgetter('lemme', 'cgram')):
        words = list(words)

        new_words = []
        words.sort(key=operator.itemgetter('phon'))
        for _, homophones in itertools.groupby(words, key=operator.itemgetter('phon')):
            homophones = list(homophones)
            homophones.sort(key=lambda x: (x['freq'], -x['nbletters']), reverse=True)
            selected = homophones.pop(0)

            # prefer canonic form
            canonic = next((x for x in homophones if x['mot'] == selected['lemme']), None)
            if canonic:
                canonic['freq'] = selected['freq']
                selected = canonic

            new_words.append(selected)

        new_words.sort(key=operator.itemgetter('freq'), reverse=True)
        new_data.extend(new_words[:max_lemme_variants])

    return new_data


def select_words(data, max_letters, decodable=False, limit=None):
    """On selectionne les mots les plus frequent en supprimant les
    homophones, les homographes, et eventuellement les mots pouvant
    former des combinaisons"""

    data.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
    known_phon = []
    known_graph = []
    new_data = []
    forbidden = []
    count = 0

    for word in data:
        if word['phon'] in known_phon:
            continue
        if word['mot'] in known_graph:
            continue
        if word['mot'] in forbidden:
            continue
        if decodable and exists_as_combinaison(word['mot'], known_graph):
            continue

        new_data.append(word)

        count = count + 1
        if limit and count >= limit:
            break

        known_phon.append(word['phon'])
        known_graph.append(word['mot'])

        if decodable:
            forbidden.extend([word['mot'] + w for w in known_graph
                              if len(word['mot'] + w) <= max_letters])
            forbidden.extend([w + word['mot'] for w in known_graph
                              if len(w + word['mot']) <= max_letters])

    return new_data


def exists_as_combinaison(word, wordlist):
    if len(wordlist) == 0:
        return False

    wordlen = len(word)
    wordlist = sorted(wordlist, key=len)
    minlen = len(wordlist[0])
    maxlen = len(wordlist[-1])
    max_candidate_len = maxlen - wordlen

    if minlen > max_candidate_len:
        return False

    for w in (w for w in wordlist + [word] if len(w) <= max_candidate_len):
        if word + w in wordlist or w + word in wordlist:
            return True

    return False


def select_typo_resistance(data, prefix_size=None, min_dist=None):
    if min_dist:
        try:
            from Levenshtein import distance
        except ImportError:
            min_dist = None
            warn('Could not import Levenshtein (missing python-levenshtein?),'
                 ' ignoring edit distance requirement')

    data.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
    known_pref = []
    new_data = []
    for word in data:
        if prefix_size:
            pref = word['mot'][:prefix_size]
            if pref in known_pref:
                continue

        if min_dist:
            is_ok = True
            for w in new_data:
                if distance(w['mot'], word['mot']) < min_dist:
                    is_ok = False
                    break

            if not is_ok:
                continue

        if prefix_size:
            known_pref.append(pref)

        new_data.append(word)

    return new_data


def print_diceware_list(words, outfile, limit=None, nb_dices=None, nb_sides=6, print_dices=True):
    if limit:
        if len(words) < limit:
            warn('not enough words')

        words.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
        words = words[:limit]

    nb_words = len(words)

    if print_dices:
        if nb_sides == 10:
            # Let's assume 10 sides means 0-9 to keep it at one character
            sides = [ str(side) for side in range(0, nb_sides) ]
        else:
            sides = [ str(side) for side in range(1, nb_sides + 1) ]

        sides_len = max(map(len, sides))
        print(sides_len)
        sides = [ '{:>{sides_len}}'.format(side, sides_len=sides_len) for side in sides ]
        if sides_len == 1:
            dice_sep = ''
        else:
            dice_sep = '-'

        if not nb_dices:
            nb_dices = math.ceil(math.log(nb_words, nb_sides))
        iters = [ iter(sides) for _ in range(nb_dices) ]
        dices = itertools.product(*iters)

    try:
        locale.setlocale(locale.LC_COLLATE, 'fr_FR.UTF-8')
    except locale.Error:
        warn('failed to set LC_COLLATE to fr_FR.UTF-8, sorting with default locale')

    words.sort(key=lambda word: locale.strxfrm(word['mot']))

    len_total = 0
    count_nonascii = 0
    for word in words:
        len_total = len_total + len(word['mot'])
        if re.search('[^a-z]', word['mot']):
            count_nonascii = count_nonascii + 1

        if print_dices:
            print('{} {}'.format(dice_sep.join(next(dices)), word['mot']), file=outfile)
        else:
            print(word['mot'], file=outfile)

    avg_len = len_total / nb_words
    efficiency = round(math.log(nb_words, 2) / avg_len, 1)

    print('\n{} words ({} non ascii), avg len: {} char., efficiency: ~{} bits / char'.format(
        nb_words, count_nonascii, round(avg_len, 2), efficiency), file=sys.stderr)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('LEXIQUE_FILE', type=argparse.FileType('r'))

    lenargs = parser.add_mutually_exclusive_group()
    lenargs.add_argument('-n', '--nb-dices', type=int, default=5)
    lenargs.add_argument('--limit', type=int)

    parser.add_argument('-c', '--char-regex', default='[a-z]')
    parser.add_argument('-M', '--max-letters', type=int, default=8)
    parser.add_argument('-m', '--min-letters', type=int, default=3)
    parser.add_argument('-a', '--non-ascii-cost', type=int, default=1)
    parser.add_argument('-s', '--skip-cgrams', default='ONO')
    parser.add_argument('-V', '--max-lemme-variants', type=int, default=None)
    parser.add_argument('-X', '--include-bad-words', action='count', default=0)
    parser.add_argument('-p', '--unique-prefix-size', type=int)
    parser.add_argument('-l', '--edit-distance', type=int)
    parser.add_argument('-d', '--decodable', action='store_true')
    parser.add_argument('--no-print-dices', action='store_true')
    parser.add_argument('--dice-sides', type=int, default=6)
    parser.add_argument('-o', '--out', type=argparse.FileType('w'))
    parser.add_argument('-x', '--exclude-file', type=argparse.FileType('r'))

    args = parser.parse_args()

    nb_dice_sides = args.dice_sides

    print_dices = not args.no_print_dices
    skip_cgrams = args.skip_cgrams.split(',')

    nb_dices = args.nb_dices
    limit = nb_dice_sides**nb_dices
    if args.limit is not None:
        nb_dices = None
        limit = args.limit

    if args.include_bad_words >= 2:
        skip_lemmes = set()
    elif args.include_bad_words == 1:
        skip_lemmes = INSULT_LEMMES
    else:
        skip_lemmes = VULGAR_LEMMES.union(INSULT_LEMMES).union(MISC_LEMMES)

    data = load_lexi_file(args.LEXIQUE_FILE, min_letters=args.min_letters,
                          max_letters=args.max_letters,
                          valid_chars=args.char_regex,
                          skip_lemmes=skip_lemmes, min_freq=0.1,
                          skip_cgrams=skip_cgrams,
                          non_ascii_cost=args.non_ascii_cost,
                          exclude_file=args.exclude_file)

    data = clean_lemmes(data, max_lemme_variants=args.max_lemme_variants)

    if args.unique_prefix_size or args.edit_distance:
        data = select_typo_resistance(data, args.unique_prefix_size, args.edit_distance)

    data = select_words(data, args.max_letters, args.decodable, limit)

    print_diceware_list(data, args.out, limit=limit, nb_dices=nb_dices,
                        nb_sides=nb_dice_sides, print_dices=print_dices)


if __name__ == '__main__':
    main()
